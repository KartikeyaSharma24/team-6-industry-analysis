{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c4aba40-1345-4d48-9678-56221af32525",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.235.42.197:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MySparkApp</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x213c84a4f90>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad259f6f-76b0-4ec2-9e47-54b77cd28cf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (3113300839.py, line 64)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdf = spark.read.option(\"header\", \"true\").csv(\"C:\\Users\\ssardana\\OneDrive - Capgemini\\Desktop\\OneDrive_1_23-6-2025 (1)\\Industry_Analysis_Src01.csv\")\u001b[39m\n                                                                                                                                                      ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import DoubleType\n",
    "\n",
    "# # Initialize Spark session\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"CompanyGrowthDF\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# # Step 1: Read CSV file\n",
    "# df = spark.read.option(\"header\", \"true\").csv(r\"C:\\Users\\ssardana\\OneDrive - Capgemini\\Desktop\\OneDrive_1_23-6-2025 (1)\\Industry_Analysis_Src01.csv\")\n",
    "\n",
    "# # Debug: Check schema and sample data\n",
    "# df.printSchema()\n",
    "# df.show(5, truncate=False)\n",
    "\n",
    "# # Step 2: Clean and transform columns\n",
    "# df_clean = df.withColumn(\"Market_Capital\", F.col(\"Market_Capital(in Million)\").cast(DoubleType())) \\\n",
    "#     .withColumn(\"Formatted_Date\", F.to_date(F.col(\"Date\"), \"dd-MM-yyyy\")) \\\n",
    "#     .withColumn(\"Year\", F.year(F.col(\"Formatted_Date\"))) \\\n",
    "#     .select(\"Company Name\", \"Year\", \"Market_Capital\")\n",
    "\n",
    "# # Debug: Verify cleaned data\n",
    "# df_clean.show(5, truncate=False)\n",
    "\n",
    "# # Step 3: Filter for years and non-null Market_Capital\n",
    "# df_filtered = df_clean.filter(\n",
    "#     (F.col(\"Year\").isin(2009, 2010, 2011)) & F.col(\"Market_Capital\").isNotNull()\n",
    "# )\n",
    "\n",
    "# # Step 4: Compute total Market Capital per (Company, Year)\n",
    "# agg_df = df_filtered.groupBy(\"Company Name\", \"Year\") \\\n",
    "#     .agg(F.round(F.sum(\"Market_Capital\"), 2).alias(\"Total_Market_Capital\"))\n",
    "\n",
    "# # Step 5: Pivot year columns into separate fields\n",
    "# pivoted = agg_df.groupBy(\"Company Name\").pivot(\"Year\", [2009, 2010, 2011]) \\\n",
    "#     .agg(F.first(\"Total_Market_Capital\"))\n",
    "\n",
    "# # Step 6: Compute Growth from 2009 to 2011\n",
    "# result = pivoted.withColumn(\n",
    "#     \"Growth_2009_to_2011\",\n",
    "#     F.round(((F.col(\"2011\") - F.col(\"2009\")) / F.col(\"2009\")) * 100, 2)\n",
    "# ).select(\"Company Name\", \"2009\", \"2011\", \"Growth_2009_to_2011\")\n",
    "\n",
    "# # Final Output\n",
    "# result.orderBy(\"Company Name\").show(truncate=False)\n",
    "\n",
    "# # Stop Spark session\n",
    "# spark.stop()\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CompanyGrowthByQuarter\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"C:\\Users\\ssardana\\OneDrive - Capgemini\\Desktop\\OneDrive_1_23-6-2025 (1)\\Industry_Analysis_Src01.csv\")\n",
    "\n",
    "# Clean and transform relevant columns\n",
    "df_clean = df.withColumn(\"Market_Capital\", F.col(\"Market_Capital(in Million)\").cast(DoubleType())) \\\n",
    "             .select(\"Company Name\", \"Quarter\", \"Market_Capital\")\n",
    "\n",
    "# Filter out rows with nulls and focus on Q1 and Q4\n",
    "df_filtered = df_clean.filter(F.col(\"Market_Capital\").isNotNull() & F.col(\"Quarter\").isin(\"Q1\", \"Q4\"))\n",
    "\n",
    "# Aggregate total market capital per company per quarter\n",
    "agg_df = df_filtered.groupBy(\"Company Name\", \"Quarter\") \\\n",
    "                    .agg(F.round(F.sum(\"Market_Capital\"), 2).alias(\"Total_Market_Capital\"))\n",
    "\n",
    "# Pivot to get Q1 and Q4 in separate columns\n",
    "pivoted = agg_df.groupBy(\"Company Name\").pivot(\"Quarter\", [\"Q1\", \"Q4\"]) \\\n",
    "                .agg(F.first(\"Total_Market_Capital\"))\n",
    "\n",
    "# Calculate growth from Q1 to Q4\n",
    "result = pivoted.withColumn(\n",
    "    \"Growth_Q1_to_Q4\",\n",
    "    F.round(((F.col(\"Q4\") - F.col(\"Q1\")) / F.col(\"Q1\")) * 100, 2)\n",
    ").select(\"Company Name\", \"Q1\", \"Q4\", \"Growth_Q1_to_Q4\")\n",
    "\n",
    "# Show the result\n",
    "result.orderBy(\"Company Name\").show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56f27e8f-816f-4329-9249-de835d05f086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+--------+--------+---------------+\n",
      "|Company Name                  |Q1      |Q4      |Growth_Q1_to_Q4|\n",
      "+------------------------------+--------+--------+---------------+\n",
      "|Aedes                         |7684.8  |7683.0  |-0.02          |\n",
      "|Allied Properties (HK) Ltd.   |24425.3 |23042.0 |-5.66          |\n",
      "|Anna Infrastructures Ltd      |5873.7  |5954.7  |1.38           |\n",
      "|Arihant Foundations & Hou     |7642.8  |7733.0  |1.18           |\n",
      "|Aspen Group Ltd               |7669.8  |8066.8  |5.18           |\n",
      "|Audi AG                       |24308.6 |30776.6 |26.61          |\n",
      "|BSEL Infrastructure Realty Ltd|7652.0  |7669.8  |0.23           |\n",
      "|BYD Co Ltd                    |12993.2 |14019.4 |7.9            |\n",
      "|Bharat Petroleum Corp Ltd     |4885.8  |5051.0  |3.38           |\n",
      "|Brookfield Asset              |3937.8  |4184.3  |6.26           |\n",
      "|CLP Holdings Limited          |6107.0  |6344.5  |3.89           |\n",
      "|Cognizant                     |7227.0  |7545.0  |4.4            |\n",
      "|Computacenter PLC             |7727.0  |7456.0  |-3.51          |\n",
      "|Dominion Resources Inc.       |4661.4  |4992.0  |7.09           |\n",
      "|Dongfeng Motor Group Co Ltd   |6818.0  |6493.0  |-4.77          |\n",
      "|Dr. Reddy's                   |5097.0  |5202.8  |2.08           |\n",
      "|Fiat Group                    |8672.0  |8525.0  |-1.7           |\n",
      "|Ford Motor                    |49561.19|50433.36|1.76           |\n",
      "|Fortune Oil PLC               |23981.6 |22907.3 |-4.48          |\n",
      "|GCL-Poly Energy               |6100.5  |6149.0  |0.8            |\n",
      "+------------------------------+--------+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CompanyGrowthByQuarter\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset using raw string to avoid unicode escape errors\n",
    "df = spark.read.option(\"header\", \"true\").csv(r\"C:\\Users\\ssardana\\OneDrive - Capgemini\\Desktop\\OneDrive_1_23-6-2025 (1)\\Industry_Analysis_Src01.csv\")\n",
    "\n",
    "# Clean and transform relevant columns\n",
    "df_clean = df.withColumn(\"Market_Capital\", F.col(\"Market_Capital(in Million)\").cast(DoubleType())) \\\n",
    "             .select(\"Company Name\", \"Quarter\", \"Market_Capital\")\n",
    "\n",
    "# Filter out rows with nulls and focus on Q1 and Q4\n",
    "df_filtered = df_clean.filter(F.col(\"Market_Capital\").isNotNull() & F.col(\"Quarter\").isin(\"Q1\", \"Q4\"))\n",
    "\n",
    "# Aggregate total market capital per company per quarter\n",
    "agg_df = df_filtered.groupBy(\"Company Name\", \"Quarter\") \\\n",
    "                    .agg(F.round(F.sum(\"Market_Capital\"), 2).alias(\"Total_Market_Capital\"))\n",
    "\n",
    "# Pivot to get Q1 and Q4 in separate columns\n",
    "pivoted = agg_df.groupBy(\"Company Name\").pivot(\"Quarter\", [\"Q1\", \"Q4\"]) \\\n",
    "                .agg(F.first(\"Total_Market_Capital\"))\n",
    "\n",
    "# Calculate growth from Q1 to Q4\n",
    "result = pivoted.withColumn(\n",
    "    \"Growth_Q1_to_Q4\",\n",
    "    F.round(((F.col(\"Q4\") - F.col(\"Q1\")) / F.col(\"Q1\")) * 100, 2)\n",
    ").select(\"Company Name\", \"Q1\", \"Q4\", \"Growth_Q1_to_Q4\")\n",
    "\n",
    "# Show the result\n",
    "result.orderBy(\"Company Name\").show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7891cf6-d8fd-40d1-9c50-1256e5b1d063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+----+------+------+------+-------+---------------+\n",
      "|Company Name                  |Year|Q1    |Q2    |Q3    |Q4     |Growth_Q1_to_Q4|\n",
      "+------------------------------+----+------+------+------+-------+---------------+\n",
      "|Aedes                         |2009|2543.8|2561.0|2591.0|2513.0 |-1.21          |\n",
      "|Aedes                         |2010|2555.0|2532.0|2600.0|2584.0 |1.14           |\n",
      "|Aedes                         |2011|2586.0|2510.8|2584.0|2586.0 |0.0            |\n",
      "|Allied Properties (HK) Ltd.   |2009|7792.3|8441.0|7099.0|8048.0 |3.28           |\n",
      "|Allied Properties (HK) Ltd.   |2010|8694.0|6837.1|7977.8|7497.0 |-13.77         |\n",
      "|Allied Properties (HK) Ltd.   |2011|7939.0|7280.8|7977.8|7497.0 |-5.57          |\n",
      "|Anna Infrastructures Ltd      |2009|1964.7|1968.0|2028.0|1942.0 |-1.16          |\n",
      "|Anna Infrastructures Ltd      |2010|1967.0|1972.0|1968.0|2028.0 |3.1            |\n",
      "|Anna Infrastructures Ltd      |2011|1942.0|1972.0|1968.0|1984.7 |2.2            |\n",
      "|Arihant Foundations & Hou     |2009|2515.8|2550.0|2571.0|2568.0 |2.07           |\n",
      "|Arihant Foundations & Hou     |2010|2544.0|2554.0|2533.0|2582.0 |1.49           |\n",
      "|Arihant Foundations & Hou     |2011|2583.0|2564.0|2623.0|2583.0 |0.0            |\n",
      "|Aspen Group Ltd               |2009|2592.6|2524.0|2626.0|2620.0 |1.06           |\n",
      "|Aspen Group Ltd               |2010|2484.6|2564.0|2623.0|2583.0 |3.96           |\n",
      "|Aspen Group Ltd               |2011|2592.6|2524.0|2635.6|2863.8 |10.46          |\n",
      "|Audi AG                       |2009|7495.8|7820.0|7099.0|8161.0 |8.87           |\n",
      "|Audi AG                       |2010|7389.0|7688.0|7786.0|13982.0|89.23          |\n",
      "|Audi AG                       |2011|9423.8|7479.0|7478.0|8633.6 |-8.39          |\n",
      "|BSEL Infrastructure Realty Ltd|2009|2564.0|2623.0|2583.0|2592.6 |1.12           |\n",
      "|BSEL Infrastructure Realty Ltd|2010|2524.0|2626.0|2620.0|2484.6 |-1.56          |\n",
      "+------------------------------+----+------+------+------+-------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CompanyQuarterlyGrowth\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load the dataset\n",
    "df = spark.read.option(\"header\", \"true\").csv(r\"C:\\Users\\ssardana\\OneDrive - Capgemini\\Desktop\\OneDrive_1_23-6-2025 (1)\\Industry_Analysis_Src01.csv\")\n",
    "\n",
    "# Convert Market Capital to numeric and extract year from Date\n",
    "df_clean = df.withColumn(\"Market_Capital\", F.col(\"Market_Capital(in Million)\").cast(DoubleType())) \\\n",
    "             .withColumn(\"ParsedDate\", F.to_date(F.col(\"Date\"), \"d-MMM-yy\")) \\\n",
    "             .withColumn(\"Year\", F.year(\"ParsedDate\")) \\\n",
    "             .select(\"Company Name\", \"Year\", \"Quarter\", \"Market_Capital\")\n",
    "\n",
    "# Filter out rows with nulls\n",
    "df_filtered = df_clean.filter(F.col(\"Market_Capital\").isNotNull())\n",
    "\n",
    "# Aggregate total market capital per company per year and quarter\n",
    "agg_df = df_filtered.groupBy(\"Company Name\", \"Year\", \"Quarter\") \\\n",
    "                    .agg(F.round(F.sum(\"Market_Capital\"), 2).alias(\"Total_Market_Capital\"))\n",
    "\n",
    "# Pivot to get Q1, Q2, Q3, Q4 in separate columns\n",
    "pivoted = agg_df.groupBy(\"Company Name\", \"Year\").pivot(\"Quarter\", [\"Q1\", \"Q2\", \"Q3\", \"Q4\"]) \\\n",
    "                .agg(F.first(\"Total_Market_Capital\"))\n",
    "\n",
    "# Calculate growth from Q1 to Q4 within each year\n",
    "result = pivoted.withColumn(\n",
    "    \"Growth_Q1_to_Q4\",\n",
    "    F.round(((F.col(\"Q4\") - F.col(\"Q1\")) / F.col(\"Q1\")) * 100, 2)\n",
    ").select(\"Company Name\", \"Year\", \"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Growth_Q1_to_Q4\")\n",
    "\n",
    "# Show the result\n",
    "result.orderBy(\"Company Name\", \"Year\").show(truncate=False)\n",
    "\n",
    "# Stop Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6689149-571c-469d-9e9e-a77d987419ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
